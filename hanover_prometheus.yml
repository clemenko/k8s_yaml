# used as a guide https://github.com/kayrus/prometheus-kubernetes
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-k8s
  namespace: monitoring
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  template:
    metadata:
      labels:
        app: node-exporter
      name: node-exporter
    spec:
      serviceAccountName: prometheus-k8s
      containers:
      - image: prom/node-exporter
        name: node-exporter
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: scrape
        volumeMounts:
        - name: data-disk
          mountPath: /data-disk
          readOnly: true
        - name: root-disk
          mountPath: /root-disk
          readOnly: true
      volumes:
        - name: data-disk
          hostPath:
            path: /localdata
        - name: root-disk
          hostPath:
            path: /
      hostNetwork: true
      hostPID: true
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: cadvisor
  namespace: monitoring
  labels:
    app: cadvisor
spec:
  selector:
    matchLabels:
      name: cadvisor
  template:
    metadata:
      labels:
        name: cadvisor
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: cadvisor
        image: google/cadvisor:latest
        volumeMounts:
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
        - name: var-run
          mountPath: /var/run
          readOnly: false
        - name: sys
          mountPath: /sys
          readOnly: true
        - name: docker
          mountPath: /var/lib/docker
          readOnly: true
        ports:
          - name: http
            containerPort: 8080
            hostPort: 4194
            protocol: TCP
        args:
          - --housekeeping_interval=10s
      terminationGracePeriodSeconds: 30
      volumes:
      - name: rootfs
        hostPath:
          path: /
      - name: var-run
        hostPath:
          path: /var/run
      - name: sys
        hostPath:
          path: /sys
      - name: docker
        hostPath:
          path: /var/lib/docker
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/path: '/alertmanager/metrics'
  labels:
    name: alertmanager
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  #type: NodePort
  ports:
  - name: alertmanager
    protocol: TCP
    port: 9093
    targetPort: 9093
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: alertmanager
  namespace: monitoring
data:
  config.yml: |-
    global:
      # ResolveTimeout is the time after which an alert is declared resolved
      # if it has not been updated.
      resolve_timeout: 5m

      # The smarthost and SMTP sender used for mail notifications.
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'foo@bar.com'
      smtp_auth_username: 'foo@bar.com'
      smtp_auth_password: 'barfoo'

      # The API URL to use for Slack notifications.
      slack_api_url: 'https://hooks.slack.com/services/abc123'

      # # The auth token for Hipchat.
      # hipchat_auth_token: '1234556789'
      #
      # # Alternative host for Hipchat.
      # hipchat_url: 'https://hipchat.foobar.org/'

    # # The directory from which notification templates are read.
    templates:
    - '/etc/alertmanager-templates/*.tmpl'

    # The root route on which each incoming alert enters.
    route:

      # The labels by which incoming alerts are grouped together. For example,
      # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
      # be batched into a single group.

      group_by: ['alertname', 'cluster', 'service']

      # When a new group of alerts is created by an incoming alert, wait at
      # least 'group_wait' to send the initial notification.
      # This way ensures that you get multiple alerts for the same group that start
      # firing shortly after another are batched together on the first
      # notification.

      group_wait: 30s

      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.

      group_interval: 5m

      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.

      #repeat_interval: 1m
      repeat_interval: 15m

      # A default receiver

      # If an alert isn't caught by a route, send it to default.
      receiver: default

      # All the above attributes are inherited by all child routes and can
      # overwritten on each.

      # The child route trees.
      routes:
      # Send severity=slack alerts to slack.
      - match:
          severity: slack
        receiver: slack_alert
      - match:
          severity: email
        receiver: slack_alert
    #   receiver: email_alert

    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#devops'
        text: '<!channel>{{ template "slack.devops.text" . }}'
        send_resolved: true

    - name: 'slack_alert'
      slack_configs:
      - channel: '#devops'
        send_resolved: true

        # # Whether or not to notify about resolved alerts.
        # send_resolved: true
        #
        # # The Slack webhook URL.
        # [ api_url: <string> | default = global.slack_api_url ]
        #
        # # The channel or user to send notifications to.
        # channel: <tmpl_string>
        #
        # # API request data as defined by the Slack webhook API.
        # [ color: <tmpl_string> | default = '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}' ]
        # [ username: <tmpl_string> | default = '{{ template "slack.default.username" . }}'
        # [ title: <tmpl_string> | default = '{{ template "slack.default.title" . }}' ]
        # [ title_link: <tmpl_string> | default = '{{ template "slack.default.titlelink" . }}' ]
        # [ icon_emoji: <tmpl_string> ]
        # [ pretext: <tmpl_string> | default = '{{ template "slack.default.pretext" . }}' ]
        # [ text: <tmpl_string> | default = '{{ template "slack.default.text" . }}' ]
        # [ fallback: <tmpl_string> | default = '{{ template "slack.default.fallback" . }}' ]

    - name: 'email_alert'
      email_configs:
      - to: 'foo@bar.com'


    #
    #
    #
    # global:
    #   # The smarthost and SMTP sender used for mail notifications.
    #   smtp_smarthost: 'localhost:25'
    #   smtp_from: 'alertmanager@example.org'
    #   smtp_auth_username: 'alertmanager'
    #   smtp_auth_password: 'password'
    #   # The auth token for Hipchat.
    #   hipchat_auth_token: '1234556789'
    #   # Alternative host for Hipchat.
    #   hipchat_url: 'https://hipchat.foobar.org/'
    #
    # # The directory from which notification templates are read.
    # templates:
    # - '/etc/alertmanager/template/*.tmpl'
    #
    # # The root route on which each incoming alert enters.
    # route:
    #   # The labels by which incoming alerts are grouped together. For example,
    #   # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
    #   # be batched into a single group.
    #   group_by: ['alertname', 'cluster', 'service']
    #
    #   # When a new group of alerts is created by an incoming alert, wait at
    #   # least 'group_wait' to send the initial notification.
    #   # This way ensures that you get multiple alerts for the same group that start
    #   # firing shortly after another are batched together on the first
    #   # notification.
    #   group_wait: 30s
    #
    #   # When the first notification was sent, wait 'group_interval' to send a batch
    #   # of new alerts that started firing for that group.
    #   group_interval: 5m
    #
    #   # If an alert has successfully been sent, wait 'repeat_interval' to
    #   # resend them.
    #   repeat_interval: 3h
    #
    #   # A default receiver
    #   receiver: team-X-mails
    #
    #   # All the above attributes are inherited by all child routes and can
    #   # overwritten on each.
    #
    #   # The child route trees.
    #   routes:
    #   # This routes performs a regular expression match on alert labels to
    #   # catch alerts that are related to a list of services.
    #   - match_re:
    #       service: ^(foo1|foo2|baz)$
    #     receiver: team-X-mails
    #     # The service has a sub-route for critical alerts, any alerts
    #     # that do not match, i.e. severity != critical, fall-back to the
    #     # parent node and are sent to 'team-X-mails'
    #     routes:
    #     - match:
    #         severity: critical
    #       receiver: team-X-pager
    #   - match:
    #       service: files
    #     receiver: team-Y-mails
    #
    #     routes:
    #     - match:
    #         severity: critical
    #       receiver: team-Y-pager
    #
    #   # This route handles all alerts coming from a database service. If there's
    #   # no team to handle it, it defaults to the DB team.
    #   - match:
    #       service: database
    #     receiver: team-DB-pager
    #     # Also group alerts by affected database.
    #     group_by: [alertname, cluster, database]
    #     routes:
    #     - match:
    #         owner: team-X
    #       receiver: team-X-pager
    #     - match:
    #         owner: team-Y
    #       receiver: team-Y-pager
    #
    #
    # # Inhibition rules allow to mute a set of alerts given that another alert is
    # # firing.
    # # We use this to mute any warning-level notifications if the same alert is
    # # already critical.
    # inhibit_rules:
    # - source_match:
    #     severity: 'critical'
    #   target_match:
    #     severity: 'warning'
    #   # Apply inhibition if the alertname is the same.
    #   equal: ['alertname', 'cluster', 'service']
    #
    #
    # receivers:
    # - name: 'team-X-mails'
    #   email_configs:
    #   - to: 'team-X+alerts@example.org'
    #
    # - name: 'team-X-pager'
    #   email_configs:
    #   - to: 'team-X+alerts-critical@example.org'
    #   pagerduty_configs:
    #   - service_key: <team-X-key>
    #
    # - name: 'team-Y-mails'
    #   email_configs:
    #   - to: 'team-Y+alerts@example.org'
    #
    # - name: 'team-Y-pager'
    #   pagerduty_configs:
    #   - service_key: <team-Y-key>
    #
    # - name: 'team-DB-pager'
    #   pagerduty_configs:
    #   - service_key: <team-DB-key>
    # - name: 'team-X-hipchat'
    #   hipchat_configs:
    #   - auth_token: <auth_token>
    #     room_id: 85
    #     message_format: html
    #     notify: true
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      name: alertmanager
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:latest
        args:
          - '-config.file=/etc/alertmanager/config.yml'
          - '-storage.path=/alertmanager'
        ports:
        - name: alertmanager
          containerPort: 9093
        volumeMounts:
        - name: config-volume
          mountPath: /etc/alertmanager
        - name: templates-volume
          mountPath: /etc/alertmanager-templates
        - name: alertmanager
          mountPath: /alertmanager
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager
      - name: templates-volume
        configMap:
          name: alertmanager-templates
      - name: alertmanager
        emptyDir: {}
---
# Useful examples on how to configure Prometheus
# * https://www.weave.works/prometheus-and-kubernetes-monitoring-your-applications/
# * https://grafana.net/dashboards/162
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-configmap
data:
  prometheus.yml: |-
    # A scrape configuration for running Prometheus on a Kubernetes cluster.
    # This uses separate scrape configs for cluster components (i.e. API server, node)
    # and services to allow each to use different authentication configs.
    #
    # Kubernetes labels will be added as Prometheus labels on metrics via the
    # `labelmap` relabeling action.
    global:
    # How long until a scrape request times out.
      scrape_interval: 60s
      scrape_timeout: 30s

    rule_files:
      - "/etc/prometheus-rules/*.rules"

    # Scrape config for cluster components.
    scrape_configs:
    # etcd is living outside of our cluster and we configure
    # it directly.
    #- job_name: 'etcd'
    #  static_configs:
    #  - targets:
    #{% for host in groups['etcd'] %}
    #    - {{ hostvars[host]['ansible_' + etcd_interface].ipv4.address }}:{{ etcd_client_port }}
    #{% endfor %}
    #  tls_config:
    #    ca_file: /etc/etcd/ssl/ca.pem
    #    cert_file: /etc/etcd/ssl/client.pem
    #    key_file: /etc/etcd/ssl/client-key.pem
    #  scheme: https
    # Dynamic etcd rule, which helps to avoid ansible jinja2 templates
    # In order to make it work as expected it is necessary to add kubelet into all etcd nodes (and set them unschedulable if necessary)
    # Also you have to mount proper TLS keypairs in case when your etcd cluster is protected by TLS auth

    - job_name: 'prometheus'
      # Monitor the prometheus server itself and availability
      static_configs:
        - targets: ['localhost:9090']

    - job_name: 'docker'
      # temp static monitor to pull docker metrics
      static_configs:
       - targets: ['161.182.134.128:9323','161.182.134.169:9323','161.182.134.168:9323','161.182.134.129:9323','161.182.134.248:9323','161.182.134.254:9323','161.182.149.25:9323']

    - job_name: 'etcd'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - api_servers:
        - https://kubernetes.default.svc
        in_cluster: true
        role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:2379'
        target_label: __address__

    - job_name: 'kubernetes-cluster'
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration (`in_cluster` below) because discovery & scraping are two
      # separate concerns in Prometheus.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        # insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - api_servers:
        - https://kubernetes.default.svc
        in_cluster: true
        role: apiserver

    - job_name: 'kubernetes-cadvisor'

      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: http

      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration (`in_cluster` below) because discovery & scraping are two
      # separate concerns in Prometheus.
    # tls_config:
      # ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        # insecure_skip_verify: true
     # bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - api_servers:
        - https://kubernetes.default.svc
        in_cluster: true
        role: node

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      # Use cadvisor 4194 port
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:4194'
        target_label: __address__

      # Backward compatibility for Kubernetes 1.3 dashboards
      metric_relabel_configs:
      - source_labels: [io_kubernetes_container_name,container_name]
        action: replace
        regex: (.*);(.*)
        replacement: '${1}${2}'
        target_label: io_kubernetes_container_name
      - source_labels: [kubernetes_pod_name,pod_name]
        action: replace
        regex: (.*);(.*)
        replacement: '${1}${2}'
        target_label: kubernetes_pod_name
      - source_labels: [kubernetes_pod_name]
        action: replace
        target_label: io_kubernetes_pod_name

    - job_name: 'kubernetes-nodes'

      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https

      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration (`in_cluster` below) because discovery & scraping are two
      # separate concerns in Prometheus.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        # insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - api_servers:
        - https://kubernetes.default.svc
        in_cluster: true
        role: node

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      # Use insecure read-only HTTP 10255 port
      # More info is here: https://github.com/kayrus/kubelet-exploit
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:10255'
        target_label: __address__

      # Backward compatibility for Kubernetes 1.3 dashboards
      metric_relabel_configs:
      - source_labels: [io_kubernetes_container_name,container_name]
        action: replace
        regex: (.*);(.*)
        replacement: '${1}${2}'
        target_label: io_kubernetes_container_name
      - source_labels: [kubernetes_pod_name,pod_name]
        action: replace
        regex: (.*);(.*)
        replacement: '${1}${2}'
        target_label: kubernetes_pod_name
      - source_labels: [kubernetes_pod_name]
        action: replace
        target_label: io_kubernetes_pod_name

    - job_name: 'kubernetes-node-exporter'
      scheme: http
      kubernetes_sd_configs:
      - api_servers:
        - https://kubernetes.default.svc
        in_cluster: true
        role: node

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__meta_kubernetes_role]
        action: replace
        target_label: kubernetes_role
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__
      - source_labels: [__meta_kubernetes_node_label_kubernetes_io_hostname]
        target_label: __instance__
      # set "name" value to "job"
      - source_labels: [job]
        regex: 'kubernetes-(.*)'
        replacement: '${1}'
        target_label: name

    # Scrape config for service endpoints.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: 'kubernetes-service-endpoints'

      kubernetes_sd_configs:
      - api_servers:
        - 'https://kubernetes.default.svc'
        in_cluster: true
        role: endpoint

      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)(?::\d+);(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_service_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name

    # Example scrape config for probing services via the Blackbox Exporter.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/probe`: Only probe services that have a value of `true`
    - job_name: 'kubernetes-services'

      metrics_path: /probe
      params:
        module: [http_2xx]

      kubernetes_sd_configs:
      - api_servers:
        - 'https://kubernetes.default.svc'
        in_cluster: true
        role: service

      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
        action: keep
        regex: true
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: __address__
        replacement: blackbox
      - source_labels: [__param_target]
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_service_namespace]
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        target_label: kubernetes_name

    # Example scrape config for pods
    #
    # The relabeling allows the actual pod scrape endpoint to be configured via the
    # following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
    - job_name: 'kubernetes-pods'

      kubernetes_sd_configs:
      - api_servers:
        - 'https://kubernetes.default.svc'
        in_cluster: true
        role: pod

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: (.+):(?:\d+);(\d+)
        replacement: ${1}:${2}
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_pod_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: 'true'
  labels:
    name: prometheus-svc # ! DO NOT USE prometheus as a name: https://github.com/kubernetes/kubernetes/issues/25573
    kubernetes.io/name: "Prometheus"
  name: prometheus-svc
  namespace: monitoring
spec:
  selector:
    app: prometheus
  type: NodePort
  ports:
  - name: prometheus
    protocol: TCP
    port: 9090
    targetPort: 9090
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-env
data:
  storage-retention: 360h
  storage-memory-chunks: '1048576'
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      name: prometheus
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus-k8s
      containers:
      - name: prometheus
        image: quay.io/coreos/prometheus:latest
        args:
          - '-storage.local.retention=$(STORAGE_RETENTION)'
          - '-storage.local.memory-chunks=$(STORAGE_MEMORY_CHUNKS)'
          - '-config.file=/etc/prometheus/prometheus.yml'
          - '-alertmanager.url=http://alertmanager:9093/alertmanager'
        ports:
        - name: web
          containerPort: 9090
        env:
        - name: STORAGE_RETENTION
          valueFrom:
            configMapKeyRef:
              name: prometheus-env
              key: storage-retention
        - name: STORAGE_MEMORY_CHUNKS
          valueFrom:
            configMapKeyRef:
              name: prometheus-env
              key: storage-memory-chunks
        volumeMounts:
        - name: config-volume
          mountPath: /etc/prometheus
        - name: rules-volume
          mountPath: /etc/prometheus-rules
        - name: prometheus-data
          mountPath: /prometheus
      volumes:
      - name: config-volume
        configMap:
          name: prometheus-configmap
      - name: rules-volume
        configMap:
          name: prometheus-rules
      - name: prometheus-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    component: core
  annotations:
    prometheus.io/scrape: 'true'
spec:
  selector:
    app: grafana
    component: core
  type: NodePort
  ports:
  - name: grafana
    protocol: TCP
    port: 3000
    targetPort: 3000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    component: core
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: grafana
        component: core
    spec:
      containers:
        - image: grafana/grafana:3.1.1
          name: grafana
          # env:
          resources:
            # keep request = limit to keep this container in guaranteed class
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 100Mi
          env:
            # This variable is required to setup templates in Grafana.
              # The following env variables are required to make Grafana accessible via
              # the kubernetes api-server proxy. On production clusters, we recommend
              # removing these env variables, setup auth for grafana, and expose the grafana
              # service using a LoadBalancer or a public IP.
            - name: GF_AUTH_BASIC_ENABLED
              value: "false"
            - name: GF_AUTH_ANONYMOUS_ENABLED
              value: "true"
            - name: GF_AUTH_ANONYMOUS_ORG_ROLE
              value: Admin
            # - name: GF_SERVER_ROOT_URL
            #   value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/
          volumeMounts:
          - name: grafana-persistent-storage
            mountPath: /var
        - name: grafana-import-dashboards
          image: docker
          command: ["/bin/sh", "-c"]
          workingDir: /opt/grafana-import-dashboards
          args:
            # FIXME use kubernetes probe instead of "until curl"
            - >
              apk --update add curl ;
              until $(curl --silent --fail --show-error --output /dev/null http://localhost:3000/api/datasources); do
                printf '.' ; sleep 1 ;
              done ;
              for file in *-datasource.json ; do
                if [ -e "$file" ] ; then
                  echo "importing $file" &&
                  curl --silent --fail --show-error \
                    --request POST http://localhost:3000/api/datasources \
                    --header "Content-Type: application/json" \
                    --data-binary "@$file" ;
                  echo "" ;
                fi
              done ;
              for file in *-dashboard.json ; do
                if [ -e "$file" ] ; then
                  # wrap exported Grafana dashboard into valid json
                  echo "importing $file" &&
                  (echo '{"dashboard":';cat "$file";echo ',"inputs":[{"name":"DS_PROMETHEUS","pluginId":"prometheus","type":"datasource","value":"prometheus"}]}') | curl --silent --fail --show-error \
                    --request POST http://localhost:3000/api/dashboards/import \
                    --header "Content-Type: application/json" \
                    --data-binary @-;
                  echo "" ;
                fi
              done ;
              while true; do
                sleep 1m ;
              done
          volumeMounts:
          - name: config-volume
            mountPath: /opt/grafana-import-dashboards
      volumes:
      - name: config-volume
        configMap:
          name: grafana-import-dashboards
      - name: grafana-persistent-storage
        emptyDir: {}
